{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Txt len: 214994\n144 Unique chars\n[135  50  65 ...  61   2   0]\nInput data:  '‚ÄúThere has never been a president in the White House who has been more supportive of HBCUs and their mission than President Trump‚Äù\\n....the possible exception of another Republican President, the late, great, Abraham Lincoln...and it‚Äôs not even close. The Democrats know this, and '\nTarget data: 'There has never been a president in the White House who has been more supportive of HBCUs and their mission than President Trump‚Äù\\n....the possible exception of another Republican President, the late, great, Abraham Lincoln...and it‚Äôs not even close. The Democrats know this, and s'\nStep    0\n  input: 135 ('‚Äú')\n  expected output: 50 ('T')\nStep    1\n  input: 50 ('T')\n  expected output: 65 ('h')\nStep    2\n  input: 65 ('h')\n  expected output: 62 ('e')\nStep    3\n  input: 62 ('e')\n  expected output: 75 ('r')\nStep    4\n  input: 75 ('r')\n  expected output: 62 ('e')\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "txt = open(\"/home/retr0/tweets.txt\",\"rb\").read().decode(encoding='utf-8')\n",
    "print(\"Txt len: {}\".format(len(txt)))\n",
    "\n",
    "vocab = sorted(set(txt))\n",
    "print(\"{} Unique chars\".format(len(vocab)))\n",
    "\n",
    "unique = len(vocab)\n",
    "\n",
    "#vectorizing\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in txt])\n",
    "print(text_as_int)\n",
    "\n",
    "\n",
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 280\n",
    "examples_per_epoch = len(txt)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<BatchDataset shapes: ((64, 280), (64, 280)), types: (tf.int64, tf.int64)>\n(64, 280, 144) # (batch_size, sequence_length, vocab_size)\n"
    }
   ],
   "source": [
    "#Batches\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "#Model\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input: \n 'L! GOD BLESS THE\\n\\nRobert, we will end up stronger than ever before. Thank you!\\nWe will be guided by the wishes of Prime Minister Abe of Japan, a great friend of the United States and a man who has done a magnificent job on the Olympic Venue, as to attending the Olympic Games in J'\n\nNext Char Predictions: \n '=q‡§Ç=\"‡§ñ‡§†‡•Å‡§î‚ÄîüáÆP2‡§ºpl\\u202ft‚Äìt„Éº‡§º3uP;‡§öXbM‡§§‡•áüá≥wEO\\'@_‡§®‡§ÖkIb‚Äî#Ysh‡§≤vKWüá∫‡§°1‡§∞rüá∫‡•à‡§ò3‡§áa‡§Ø\\'.‡§èd‡•Åa.f‡§°@s?,&‡§ø ‡§∑4J‡§†‚Äì‡§®‡§ò‡§î5‚Äò!‡§µM\\u202f‡§≤Yl‡§è„Éº=‡§ºP‡§Å„Éº‡§ü‡§¶#\\n‡•Ç„Éº‡§ß‡§∞‡•à‚Äù‡§ù8@\\u202fL‡§™=‡§∑‡§°üá∫Y‡§π‚Äù‡§á‡§â‡§òjc‡•Ç‡§Çv\\n7‡§Å‡§Æ0üá≥\\'+‡§õ‡•á‡•Å‡§Ç%V‡§ÇMüá∫‡§óvüáÆ0‡§æ‡§ø‡§òX‡§ó/‡§Ü‡§â‡§îxL=;‡§®G‡§ó‡§Æw‚ÄùC-‡§ñc‡§πn‡§Ü‚Äú‡§°m‡§â0!nAüá∏N5jvs8‡§≤?üá∏PA‡§ß‡§°‡§ù‡§™R‚Äù‡§®Wq‡§∑t‡§¨DwK‡•ÄF@‚Äìk0\\'\\'‚Äù1C‚Äì‡§∑%‚ÄîSOZe\"#6\\u202fEUMlNSP‡§á\\nn‡§∏SL‡§Ç‡§î‡•Å‡§∏,a‡§¶9E‡§°-‡§ò6d‡•§‡§öB_‡§™üá∏Yu‚ÄìNU‚Äú'\n"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction shape:  (64, 280, 144)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.970621\n"
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 11 steps\nEpoch 1/10\n11/11 [==============================] - 88s 8s/step - loss: 5.0273\nEpoch 2/10\n11/11 [==============================] - 81s 7s/step - loss: 3.9735\nEpoch 3/10\n11/11 [==============================] - 81s 7s/step - loss: 3.2810\nEpoch 4/10\n11/11 [==============================] - 81s 7s/step - loss: 3.0957\nEpoch 5/10\n11/11 [==============================] - 82s 7s/step - loss: 2.9131\nEpoch 6/10\n11/11 [==============================] - 88s 8s/step - loss: 2.7651\nEpoch 7/10\n11/11 [==============================] - 82s 7s/step - loss: 2.6692\nEpoch 8/10\n11/11 [==============================] - 82s 7s/step - loss: 2.5950\nEpoch 9/10\n11/11 [==============================] - 80s 7s/step - loss: 2.5431\nEpoch 10/10\n11/11 [==============================] - 81s 7s/step - loss: 2.4949\n"
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./training_checkpoints/ckpt_10'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            36864     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 144)            147600    \n=================================================================\nTotal params: 4,122,768\nTrainable params: 4,122,768\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Thean 5. Dere &aowdsimlecags. @Sthets G He; s ho basudderren, whox they ate furibp hoon, wold he toedicucilh and thandike Ahant yada‚Äù Jong the sall Sechan. Ne memo rcrient Fes ouk sa ciust he ond toures hliwramfif Neativathe sesa bune,!\nMU.0..). Fers.. N Thanpewey ar DEY on Direnein thist weouber St oard comes, ton mouud yyen tha af buppplesingres bem.\nC in mergoft!\n@ReNeVest wate rodhat, un sibgion @pasy pmouly ind, Tfhin frekeys, Pren‚Äôtind ture at arsitind!\nTo tery vabeenteuy fheora. Nerigre &ade ther, ory als angse powe th Nedy Rasing moucle porske Fanstre ollerika pe pant ngy raverty in wigrty ur Con the ghar yt3 Wompibs Fogpif wallly avef tathang Coon wing sis Gordkoce ty yous overonke gramares if hot Netat bug Thin P Poto weint. Brmeas he whe andas, dat Cureper thesors atk nam @N, Nheade clisg is Seigitn the OPqrinmer, ths wat bor tocono ghered was, Wemer to be ing bat elin tha cof as, Fust on tr bouk! Svao dof ofly oveind therere qotasi. ny Iy fon @EIU‡•áOins yen deen s oullast tonnt\n"
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"The\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbasecondaaba6ad487cb0442fbf211c37a0da7cba",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}